In [0]:
from google.colab import drive
drive.mount('/content/gdrive')
Go to this URL in a browser: https://accounts.google.com/o/oauth2
Enter your authorization code:
··········
Mounted at /content/gdrive
In [0]:
import sys, os
import pandas as pd
import numpy as np

from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D
from keras.losses import categorical_crossentropy
from keras.optimizers import Adam
from keras.regularizers import l2
from keras.utils import np_utils
# pd.set_option('display.max_rows', 500)
# pd.set_option('display.max_columns', 500)
# pd.set_option('display.width', 1000)

df=pd.read_csv('gdrive/My Drive/fer2013.csv')

# print(df.info())
# print(df["Usage"].value_counts())

# print(df.head())
X_train,train_y,X_test,test_y=[],[],[],[]

for index, row in df.iterrows():
    val=row['pixels'].split(" ")
    try:
        if 'Training' in row['Usage']:
           X_train.append(np.array(val,'float32'))
           train_y.append(row['emotion'])
        elif 'PublicTest' in row['Usage']:
           X_test.append(np.array(val,'float32'))
           test_y.append(row['emotion'])
    except:
        print(f"error occured at index :{index} and row:{row}")


num_features = 64
num_labels = 7
batch_size = 64
epochs = 200
width, height = 48, 48


X_train = np.array(X_train,'float32')
train_y = np.array(train_y,'float32')
X_test = np.array(X_test,'float32')
test_y = np.array(test_y,'float32')

train_y=np_utils.to_categorical(train_y, num_classes=num_labels)
test_y=np_utils.to_categorical(test_y, num_classes=num_labels)

#cannot produce
#normalizing data between oand 1
X_train -= np.mean(X_train, axis=0)
X_train /= np.std(X_train, axis=0)

X_test -= np.mean(X_test, axis=0)
X_test /= np.std(X_test, axis=0)

X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)

X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)

# print(f"shape:{X_train.shape}")
##designing the cnn
#1st convolution layer
model = Sequential()

model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1:])))
model.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))
# model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))
model.add(Dropout(0.5))

#2nd convolution layer
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(Conv2D(64, (3, 3), activation='relu'))
# model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))
model.add(Dropout(0.5))

#3rd convolution layer
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(Conv2D(128, (3, 3), activation='relu'))
# model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))

model.add(Flatten())

#fully connected neural networks
model.add(Dense(1024, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1024, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(num_labels, activation='softmax'))

# model.summary()

#Compliling the model
model.compile(loss=categorical_crossentropy,
              optimizer=Adam(),
              metrics=['accuracy'])

#Training the model
model.fit(X_train, train_y,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(X_test, test_y),
          shuffle=True)


#Saving the  model to  use it later on
fer_json = model.to_json()
with open("fer.json", "w") as json_file:
    json_file.write(fer_json)
model.save_weights("fer.h5")
Train on 28709 samples, validate on 3589 samples
Epoch 1/200
28709/28709 [==============================] - 11s 369us/step - loss: 1.7274 - acc: 0.2937 - val_loss: 1.5473 - val_acc: 0.3862
Epoch 2/200
28709/28709 [==============================] - 9s 328us/step - loss: 1.5182 - acc: 0.4059 - val_loss: 1.4057 - val_acc: 0.4377
Epoch 3/200
28709/28709 [==============================] - 9s 330us/step - loss: 1.4109 - acc: 0.4553 - val_loss: 1.3337 - val_acc: 0.4806
Epoch 4/200
28709/28709 [==============================] - 9s 329us/step - loss: 1.3445 - acc: 0.4830 - val_loss: 1.2841 - val_acc: 0.4999
Epoch 5/200
28709/28709 [==============================] - 9s 330us/step - loss: 1.3077 - acc: 0.4946 - val_loss: 1.2569 - val_acc: 0.5121
Epoch 6/200
28709/28709 [==============================] - 10s 331us/step - loss: 1.2674 - acc: 0.5167 - val_loss: 1.2304 - val_acc: 0.5252
Epoch 7/200
28709/28709 [==============================] - 9s 330us/step - loss: 1.2401 - acc: 0.5230 - val_loss: 1.2394 - val_acc: 0.5244
Epoch 8/200
28709/28709 [==============================] - 9s 330us/step - loss: 1.2177 - acc: 0.5355 - val_loss: 1.2105 - val_acc: 0.5386
Epoch 9/200
28709/28709 [==============================] - 9s 330us/step - loss: 1.1868 - acc: 0.5462 - val_loss: 1.1963 - val_acc: 0.5400
Epoch 10/200
28709/28709 [==============================] - 10s 331us/step - loss: 1.1722 - acc: 0.5528 - val_loss: 1.1963 - val_acc: 0.5411
Epoch 11/200
28709/28709 [==============================] - 9s 330us/step - loss: 1.1479 - acc: 0.5605 - val_loss: 1.1797 - val_acc: 0.5489
Epoch 12/200
28709/28709 [==============================] - 10s 332us/step - loss: 1.1372 - acc: 0.5668 - val_loss: 1.1629 - val_acc: 0.5464
Epoch 13/200
28709/28709 [==============================] - 10s 332us/step - loss: 1.1145 - acc: 0.5729 - val_loss: 1.1523 - val_acc: 0.5603
Epoch 14/200
28709/28709 [==============================] - 9s 331us/step - loss: 1.0974 - acc: 0.5786 - val_loss: 1.1678 - val_acc: 0.5500
Epoch 15/200
28709/28709 [==============================] - 10s 331us/step - loss: 1.0796 - acc: 0.5877 - val_loss: 1.1758 - val_acc: 0.5492
Epoch 16/200
28709/28709 [==============================] - 9s 330us/step - loss: 1.0705 - acc: 0.5898 - val_loss: 1.1373 - val_acc: 0.5748
Epoch 17/200
28709/28709 [==============================] - 9s 331us/step - loss: 1.0452 - acc: 0.5995 - val_loss: 1.1369 - val_acc: 0.5782
Epoch 18/200
28709/28709 [==============================] - 9s 331us/step - loss: 1.0413 - acc: 0.6012 - val_loss: 1.1479 - val_acc: 0.5692
Epoch 19/200
28709/28709 [==============================] - 10s 332us/step - loss: 1.0221 - acc: 0.6118 - val_loss: 1.1457 - val_acc: 0.5709
Epoch 20/200
28709/28709 [==============================] - 10s 332us/step - loss: 1.0037 - acc: 0.6189 - val_loss: 1.1904 - val_acc: 0.5539
Epoch 21/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.9915 - acc: 0.6260 - val_loss: 1.1616 - val_acc: 0.5634
Epoch 22/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.9759 - acc: 0.6304 - val_loss: 1.1437 - val_acc: 0.5729
Epoch 23/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.9606 - acc: 0.6351 - val_loss: 1.1516 - val_acc: 0.5759
Epoch 24/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.9515 - acc: 0.6381 - val_loss: 1.1345 - val_acc: 0.5768
Epoch 25/200
28709/28709 [==============================] - 10s 333us/step - loss: 0.9292 - acc: 0.6441 - val_loss: 1.1738 - val_acc: 0.5662
Epoch 26/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.9239 - acc: 0.6481 - val_loss: 1.1826 - val_acc: 0.5790
Epoch 27/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.9038 - acc: 0.6580 - val_loss: 1.2064 - val_acc: 0.5687
Epoch 28/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.8991 - acc: 0.6570 - val_loss: 1.1781 - val_acc: 0.5695
Epoch 29/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.8785 - acc: 0.6683 - val_loss: 1.1882 - val_acc: 0.5840
Epoch 30/200
28709/28709 [==============================] - 10s 333us/step - loss: 0.8652 - acc: 0.6698 - val_loss: 1.2141 - val_acc: 0.5695
Epoch 31/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.8572 - acc: 0.6771 - val_loss: 1.2313 - val_acc: 0.5651
Epoch 32/200
28709/28709 [==============================] - 10s 333us/step - loss: 0.8416 - acc: 0.6844 - val_loss: 1.2080 - val_acc: 0.5617
Epoch 33/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.8379 - acc: 0.6876 - val_loss: 1.2147 - val_acc: 0.5704
Epoch 34/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.8231 - acc: 0.6870 - val_loss: 1.2161 - val_acc: 0.5698
Epoch 35/200
28709/28709 [==============================] - 10s 333us/step - loss: 0.8032 - acc: 0.6972 - val_loss: 1.1971 - val_acc: 0.5848
Epoch 36/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.8068 - acc: 0.6952 - val_loss: 1.2326 - val_acc: 0.5734
Epoch 37/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.7877 - acc: 0.7051 - val_loss: 1.2494 - val_acc: 0.5818
Epoch 38/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.7820 - acc: 0.7052 - val_loss: 1.2611 - val_acc: 0.5731
Epoch 39/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.7634 - acc: 0.7130 - val_loss: 1.2440 - val_acc: 0.5726
Epoch 40/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.7549 - acc: 0.7173 - val_loss: 1.2870 - val_acc: 0.5765
Epoch 41/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.7390 - acc: 0.7220 - val_loss: 1.2617 - val_acc: 0.5773
Epoch 42/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.7424 - acc: 0.7213 - val_loss: 1.2903 - val_acc: 0.5626
Epoch 43/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.7380 - acc: 0.7229 - val_loss: 1.2547 - val_acc: 0.5692
Epoch 44/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.7154 - acc: 0.7356 - val_loss: 1.3186 - val_acc: 0.5829
Epoch 45/200
28709/28709 [==============================] - 10s 334us/step - loss: 0.7089 - acc: 0.7347 - val_loss: 1.2719 - val_acc: 0.5762
Epoch 46/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.6960 - acc: 0.7418 - val_loss: 1.2883 - val_acc: 0.5756
Epoch 47/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.6930 - acc: 0.7425 - val_loss: 1.2938 - val_acc: 0.5793
Epoch 48/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.6795 - acc: 0.7477 - val_loss: 1.3409 - val_acc: 0.5762
Epoch 49/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.6664 - acc: 0.7528 - val_loss: 1.3276 - val_acc: 0.5651
Epoch 50/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.6754 - acc: 0.7499 - val_loss: 1.3510 - val_acc: 0.5784
Epoch 51/200
28709/28709 [==============================] - 10s 333us/step - loss: 0.6598 - acc: 0.7547 - val_loss: 1.3616 - val_acc: 0.5720
Epoch 52/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.6559 - acc: 0.7571 - val_loss: 1.3348 - val_acc: 0.5851
Epoch 53/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.6407 - acc: 0.7600 - val_loss: 1.3386 - val_acc: 0.5829
Epoch 54/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.6458 - acc: 0.7618 - val_loss: 1.3742 - val_acc: 0.5729
Epoch 55/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.6309 - acc: 0.7666 - val_loss: 1.3392 - val_acc: 0.5723
Epoch 56/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.6296 - acc: 0.7678 - val_loss: 1.3955 - val_acc: 0.5701
Epoch 57/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.6087 - acc: 0.7732 - val_loss: 1.3747 - val_acc: 0.5690
Epoch 58/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.6030 - acc: 0.7781 - val_loss: 1.4097 - val_acc: 0.5762
Epoch 59/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.6031 - acc: 0.7796 - val_loss: 1.4175 - val_acc: 0.5723
Epoch 60/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.5877 - acc: 0.7823 - val_loss: 1.4252 - val_acc: 0.5754
Epoch 61/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.5887 - acc: 0.7843 - val_loss: 1.4415 - val_acc: 0.5795
Epoch 62/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.5753 - acc: 0.7865 - val_loss: 1.3601 - val_acc: 0.5784
Epoch 63/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.5728 - acc: 0.7901 - val_loss: 1.4199 - val_acc: 0.5815
Epoch 64/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.5752 - acc: 0.7896 - val_loss: 1.4053 - val_acc: 0.5765
Epoch 65/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.5518 - acc: 0.7971 - val_loss: 1.5581 - val_acc: 0.5642
Epoch 66/200
28709/28709 [==============================] - 10s 333us/step - loss: 0.5536 - acc: 0.7989 - val_loss: 1.4523 - val_acc: 0.5795
Epoch 67/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.5690 - acc: 0.7932 - val_loss: 1.4408 - val_acc: 0.5751
Epoch 68/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.5530 - acc: 0.7970 - val_loss: 1.4322 - val_acc: 0.5832
Epoch 69/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.5528 - acc: 0.7999 - val_loss: 1.4578 - val_acc: 0.5829
Epoch 70/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.5347 - acc: 0.8080 - val_loss: 1.4719 - val_acc: 0.5676
Epoch 71/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.5215 - acc: 0.8106 - val_loss: 1.5091 - val_acc: 0.5670
Epoch 72/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.5323 - acc: 0.8058 - val_loss: 1.4915 - val_acc: 0.5701
Epoch 73/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.5139 - acc: 0.8135 - val_loss: 1.4910 - val_acc: 0.5745
Epoch 74/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.5185 - acc: 0.8105 - val_loss: 1.5253 - val_acc: 0.5759
Epoch 75/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.5214 - acc: 0.8094 - val_loss: 1.4636 - val_acc: 0.5795
Epoch 76/200
28709/28709 [==============================] - 10s 333us/step - loss: 0.5134 - acc: 0.8147 - val_loss: 1.4954 - val_acc: 0.5698
Epoch 77/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.5117 - acc: 0.8141 - val_loss: 1.4484 - val_acc: 0.5798
Epoch 78/200
28709/28709 [==============================] - 10s 333us/step - loss: 0.5071 - acc: 0.8159 - val_loss: 1.4882 - val_acc: 0.5751
Epoch 79/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.5074 - acc: 0.8172 - val_loss: 1.5797 - val_acc: 0.5770
Epoch 80/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.4962 - acc: 0.8221 - val_loss: 1.5605 - val_acc: 0.5823
Epoch 81/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.4892 - acc: 0.8245 - val_loss: 1.5344 - val_acc: 0.5712
Epoch 82/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.4834 - acc: 0.8263 - val_loss: 1.5772 - val_acc: 0.5706
Epoch 83/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.4838 - acc: 0.8266 - val_loss: 1.5195 - val_acc: 0.5673
Epoch 84/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.4790 - acc: 0.8271 - val_loss: 1.5393 - val_acc: 0.5751
Epoch 85/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.4687 - acc: 0.8314 - val_loss: 1.5671 - val_acc: 0.5798
Epoch 86/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.4554 - acc: 0.8391 - val_loss: 1.6085 - val_acc: 0.5712
Epoch 87/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.4731 - acc: 0.8316 - val_loss: 1.6153 - val_acc: 0.5748
Epoch 88/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.4589 - acc: 0.8335 - val_loss: 1.6353 - val_acc: 0.5729
Epoch 89/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.4676 - acc: 0.8330 - val_loss: 1.5874 - val_acc: 0.5765
Epoch 90/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.4436 - acc: 0.8390 - val_loss: 1.6443 - val_acc: 0.5695
Epoch 91/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.4584 - acc: 0.8386 - val_loss: 1.6240 - val_acc: 0.5784
Epoch 92/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.4514 - acc: 0.8396 - val_loss: 1.6563 - val_acc: 0.5639
Epoch 93/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.4431 - acc: 0.8417 - val_loss: 1.6037 - val_acc: 0.5639
Epoch 94/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.4421 - acc: 0.8411 - val_loss: 1.5579 - val_acc: 0.5720
Epoch 95/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.4364 - acc: 0.8408 - val_loss: 1.5404 - val_acc: 0.5695
Epoch 96/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.4245 - acc: 0.8501 - val_loss: 1.6775 - val_acc: 0.5720
Epoch 97/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.4300 - acc: 0.8464 - val_loss: 1.6338 - val_acc: 0.5712
Epoch 98/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.4229 - acc: 0.8503 - val_loss: 1.6549 - val_acc: 0.5701
Epoch 99/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.4283 - acc: 0.8474 - val_loss: 1.6329 - val_acc: 0.5770
Epoch 100/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.4192 - acc: 0.8508 - val_loss: 1.6416 - val_acc: 0.5684
Epoch 101/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.4247 - acc: 0.8496 - val_loss: 1.6028 - val_acc: 0.5779
Epoch 102/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.4169 - acc: 0.8526 - val_loss: 1.6196 - val_acc: 0.5745
Epoch 103/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.4155 - acc: 0.8527 - val_loss: 1.6845 - val_acc: 0.5678
Epoch 104/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.4220 - acc: 0.8510 - val_loss: 1.6650 - val_acc: 0.5692
Epoch 105/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.4110 - acc: 0.8553 - val_loss: 1.6100 - val_acc: 0.5620
Epoch 106/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.4232 - acc: 0.8521 - val_loss: 1.6323 - val_acc: 0.5656
Epoch 107/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.4117 - acc: 0.8546 - val_loss: 1.6696 - val_acc: 0.5759
Epoch 108/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.3984 - acc: 0.8591 - val_loss: 1.7669 - val_acc: 0.5642
Epoch 109/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3955 - acc: 0.8600 - val_loss: 1.6740 - val_acc: 0.5782
Epoch 110/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.4014 - acc: 0.8593 - val_loss: 1.7157 - val_acc: 0.5709
Epoch 111/200
28709/28709 [==============================] - 10s 333us/step - loss: 0.3938 - acc: 0.8631 - val_loss: 1.7009 - val_acc: 0.5795
Epoch 112/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.4044 - acc: 0.8573 - val_loss: 1.7021 - val_acc: 0.5690
Epoch 113/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.3923 - acc: 0.8630 - val_loss: 1.7301 - val_acc: 0.5729
Epoch 114/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3825 - acc: 0.8681 - val_loss: 1.8037 - val_acc: 0.5639
Epoch 115/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.3918 - acc: 0.8634 - val_loss: 1.6887 - val_acc: 0.5748
Epoch 116/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.3813 - acc: 0.8649 - val_loss: 1.8227 - val_acc: 0.5695
Epoch 117/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3810 - acc: 0.8667 - val_loss: 1.7369 - val_acc: 0.5720
Epoch 118/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3886 - acc: 0.8650 - val_loss: 1.6428 - val_acc: 0.5734
Epoch 119/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3735 - acc: 0.8714 - val_loss: 1.7403 - val_acc: 0.5687
Epoch 120/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3794 - acc: 0.8678 - val_loss: 1.6621 - val_acc: 0.5748
Epoch 121/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.3756 - acc: 0.8712 - val_loss: 1.7351 - val_acc: 0.5690
Epoch 122/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3713 - acc: 0.8713 - val_loss: 1.7025 - val_acc: 0.5717
Epoch 123/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3777 - acc: 0.8684 - val_loss: 1.6702 - val_acc: 0.5662
Epoch 124/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3656 - acc: 0.8726 - val_loss: 1.7618 - val_acc: 0.5662
Epoch 125/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.3736 - acc: 0.8716 - val_loss: 1.7357 - val_acc: 0.5784
Epoch 126/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3745 - acc: 0.8709 - val_loss: 1.7745 - val_acc: 0.5631
Epoch 127/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.3704 - acc: 0.8738 - val_loss: 1.6782 - val_acc: 0.5720
Epoch 128/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.3681 - acc: 0.8711 - val_loss: 1.7745 - val_acc: 0.5768
Epoch 129/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.3652 - acc: 0.8764 - val_loss: 1.7043 - val_acc: 0.5698
Epoch 130/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3575 - acc: 0.8789 - val_loss: 1.7609 - val_acc: 0.5659
Epoch 131/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3628 - acc: 0.8746 - val_loss: 1.7452 - val_acc: 0.5662
Epoch 132/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3650 - acc: 0.8739 - val_loss: 1.7913 - val_acc: 0.5653
Epoch 133/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3570 - acc: 0.8778 - val_loss: 1.7720 - val_acc: 0.5684
Epoch 134/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.3539 - acc: 0.8776 - val_loss: 1.8067 - val_acc: 0.5768
Epoch 135/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3478 - acc: 0.8788 - val_loss: 1.8158 - val_acc: 0.5656
Epoch 136/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3607 - acc: 0.8773 - val_loss: 1.8087 - val_acc: 0.5706
Epoch 137/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3531 - acc: 0.8778 - val_loss: 1.8008 - val_acc: 0.5692
Epoch 138/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3560 - acc: 0.8782 - val_loss: 1.7446 - val_acc: 0.5793
Epoch 139/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3442 - acc: 0.8815 - val_loss: 1.7783 - val_acc: 0.5692
Epoch 140/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3486 - acc: 0.8818 - val_loss: 1.8196 - val_acc: 0.5726
Epoch 141/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.3368 - acc: 0.8845 - val_loss: 1.8001 - val_acc: 0.5706
Epoch 142/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3447 - acc: 0.8815 - val_loss: 1.8024 - val_acc: 0.5676
Epoch 143/200
28709/28709 [==============================] - 10s 333us/step - loss: 0.3390 - acc: 0.8818 - val_loss: 1.7858 - val_acc: 0.5692
Epoch 144/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.3513 - acc: 0.8807 - val_loss: 1.8013 - val_acc: 0.5776
Epoch 145/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3389 - acc: 0.8830 - val_loss: 1.8794 - val_acc: 0.5709
Epoch 146/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3366 - acc: 0.8824 - val_loss: 1.8769 - val_acc: 0.5717
Epoch 147/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.3298 - acc: 0.8852 - val_loss: 1.7870 - val_acc: 0.5748
Epoch 148/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.3360 - acc: 0.8853 - val_loss: 1.8689 - val_acc: 0.5804
Epoch 149/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3256 - acc: 0.8862 - val_loss: 1.8716 - val_acc: 0.5706
Epoch 150/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.3356 - acc: 0.8855 - val_loss: 1.7906 - val_acc: 0.5773
Epoch 151/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3362 - acc: 0.8850 - val_loss: 1.8127 - val_acc: 0.5756
Epoch 152/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3231 - acc: 0.8931 - val_loss: 1.8828 - val_acc: 0.5690
Epoch 153/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.3256 - acc: 0.8898 - val_loss: 1.8626 - val_acc: 0.5648
Epoch 154/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3262 - acc: 0.8885 - val_loss: 1.8027 - val_acc: 0.5717
Epoch 155/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3296 - acc: 0.8874 - val_loss: 1.9014 - val_acc: 0.5740
Epoch 156/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3266 - acc: 0.8879 - val_loss: 1.7987 - val_acc: 0.5653
Epoch 157/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3249 - acc: 0.8895 - val_loss: 1.8893 - val_acc: 0.5592
Epoch 158/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3270 - acc: 0.8884 - val_loss: 1.7996 - val_acc: 0.5659
Epoch 159/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3291 - acc: 0.8897 - val_loss: 1.7796 - val_acc: 0.5667
Epoch 160/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3385 - acc: 0.8862 - val_loss: 1.8301 - val_acc: 0.5715
Epoch 161/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.3316 - acc: 0.8862 - val_loss: 1.8239 - val_acc: 0.5704
Epoch 162/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3125 - acc: 0.8938 - val_loss: 1.8945 - val_acc: 0.5561
Epoch 163/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.3243 - acc: 0.8885 - val_loss: 1.8810 - val_acc: 0.5645
Epoch 164/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3150 - acc: 0.8923 - val_loss: 1.8962 - val_acc: 0.5648
Epoch 165/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3243 - acc: 0.8905 - val_loss: 1.8643 - val_acc: 0.5687
Epoch 166/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3238 - acc: 0.8909 - val_loss: 1.8839 - val_acc: 0.5670
Epoch 167/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3221 - acc: 0.8921 - val_loss: 1.9199 - val_acc: 0.5698
Epoch 168/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3211 - acc: 0.8922 - val_loss: 1.8677 - val_acc: 0.5712
Epoch 169/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3125 - acc: 0.8931 - val_loss: 1.8150 - val_acc: 0.5695
Epoch 170/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.3018 - acc: 0.8967 - val_loss: 1.8350 - val_acc: 0.5651
Epoch 171/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.3157 - acc: 0.8938 - val_loss: 1.9005 - val_acc: 0.5776
Epoch 172/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3101 - acc: 0.8968 - val_loss: 1.7376 - val_acc: 0.5784
Epoch 173/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.2926 - acc: 0.9015 - val_loss: 1.9054 - val_acc: 0.5645
Epoch 174/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3104 - acc: 0.8948 - val_loss: 1.8862 - val_acc: 0.5589
Epoch 175/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3100 - acc: 0.8945 - val_loss: 1.9087 - val_acc: 0.5793
Epoch 176/200
28709/28709 [==============================] - 10s 332us/step - loss: 0.3101 - acc: 0.8964 - val_loss: 1.8904 - val_acc: 0.5743
Epoch 177/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3114 - acc: 0.8952 - val_loss: 1.9100 - val_acc: 0.5687
Epoch 178/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.3087 - acc: 0.8956 - val_loss: 1.8440 - val_acc: 0.5751
Epoch 179/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3186 - acc: 0.8949 - val_loss: 1.8768 - val_acc: 0.5642
Epoch 180/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3044 - acc: 0.8986 - val_loss: 1.9380 - val_acc: 0.5662
Epoch 181/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.2971 - acc: 0.9001 - val_loss: 1.9293 - val_acc: 0.5698
Epoch 182/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3056 - acc: 0.8958 - val_loss: 1.9033 - val_acc: 0.5631
Epoch 183/200
28709/28709 [==============================] - 10s 331us/step - loss: 0.3002 - acc: 0.8966 - val_loss: 1.9461 - val_acc: 0.5748
Epoch 184/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3042 - acc: 0.8990 - val_loss: 1.8951 - val_acc: 0.5717
Epoch 185/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3005 - acc: 0.8986 - val_loss: 1.9418 - val_acc: 0.5656
Epoch 186/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3086 - acc: 0.8991 - val_loss: 1.8882 - val_acc: 0.5659
Epoch 187/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3154 - acc: 0.8964 - val_loss: 1.9496 - val_acc: 0.5595
Epoch 188/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3068 - acc: 0.8993 - val_loss: 1.9107 - val_acc: 0.5603
Epoch 189/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.3056 - acc: 0.8986 - val_loss: 1.8563 - val_acc: 0.5681
Epoch 190/200
28709/28709 [==============================] - 9s 330us/step - loss: 0.2929 - acc: 0.9026 - val_loss: 1.9133 - val_acc: 0.5731
Epoch 191/200
28709/28709 [==============================] - 9s 331us/step - loss: 0.3019 - acc: 0.8995 - val_loss: 1.9807 - val_acc: 0.5673
Epoch 192/200
17344/28709 [=================>............] - ETA: 3s - loss: 0.2899 - acc: 0.9020
